

###################    Precision 72.4%. AUC is 84.5%    ###############################

rm(list = ls())
setwd("C:\\Users\\sohail.ahmad\\Desktop\\Charge-off Pattern\\New_22nd May")
list.files()
df1 = read.csv("Final_1st June.csv")

names(df1)
df = df1[,c(3:5,7,11)]

colSums(is.na(df))

df = na.omit(df)

df$Chargeoff_Status = as.factor(df$Chargeoff_Status)
set.seed(100)
spl = sample(nrow(df), 0.7*nrow(df))
train = df[spl,]
test = df[-spl,]

# DMwR takes factors only. So it was converted to factors above
library(DMwR)
set.seed(100)

#It will oversample the minority class by double and take majority class as one 3rd (divide majority class by 3)
train1 = SMOTE(Chargeoff_Status ~ ., train, perc.over = 100, perc.under=300)
table(train1$Chargeoff_Status)
table(test$Chargeoff_Status)

# XGB takes labels as numeric only. After converting to numeric, values become [1,2]; therefore we subtracted 1 to get [0,1]
train1$Chargeoff_Status = as.numeric(train1$Chargeoff_Status)
test$Chargeoff_Status = as.numeric(test$Chargeoff_Status)
y = train1$Chargeoff_Status
y1 = test$Chargeoff_Status
y = y-1
y1 = y1-1
train1$Chargeoff_Status = NULL
test$Chargeoff_Status = NULL

# XGB takes train data as matrix only
trainx = as.matrix(train1)
testx = as.matrix(test)

library(Matrix)
library(data.table)
library(xgboost)

#Here The output that it'll giveis the classification error on the training data set.
set.seed(100)

# nround at 88 has the minm error rate
cv.res = xgb.cv(data = trainx, nfold = 5, label = y, nround = 100,
                objective = "binary:logistic", eval_metric = "error")


# Explaination of all the parameter: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md
# eta is [default=0.3, alias: learning_rate]step size shrinkage; used in update to prevents overfitting.
# max_dpeth is the maximum depth of a tree
# nround is the max no. of iterations or no. of trees to be added to the model.
# subsample is ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to
# grow trees and this will prevent overfitting. colsample_bytree [default=1] is subsample ratio of columns when constructing each tree.
# nthread [default to maximum number of threads available if not set] is the number of parallel threads used to run xgboost

# #Here The output that it'll give, is the classification error on the training data set.

xgb = xgboost(data = trainx, 
              label = y, 
              eta = 0.2,
              max_depth = 8, 
              nround=88, 
              subsample = 0.3,
              colsample_bytree = 0.25,
              set.seed = 1,
              eval_metric = "error",
              objective = "binary:logistic",
              nthread = 2
)

importance = xgb.importance(feature_names = colnames(trainx), model = xgb)
head(importance)

Pred = predict(xgb, testx)
summary(Pred)
table(y1, Pred>=0.5)

#ROC curve and AUC of the prediction
library(ROSE)
roc.curve(y1, Pred)








