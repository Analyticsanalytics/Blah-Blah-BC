



rm(list = ls())
setwd("C:\\Users\\sohail.ahmad\\Desktop\\Sit Visits Data")
df = read.csv(file = "dataset.csv", na.strings = c("", " ", NA))
colSums(is.na(df))
names(df)

df[,c(1,4,5)] = NULL



#Converting the factors into numerical vars since 
library(Metrics)
options(na.action="na.pass")
df1 = model.matrix(~.,df)[,-1]
dfm = as.data.frame(df1)


set.seed(20)
library(caTools)
spl = sample.split(dfm$Delinquent, SplitRatio = 0.7)
train = subset(dfm, spl==TRUE)
test = subset(dfm, spl==FALSE)

#XGB takes response as a separate vector. The response should be only numeric. Start it from 0, if its classification and real values
# for regression
y = train$Delinquent
y1 = test$Delinquent
y = as.numeric(y)
y = y-1  #After converting to numeric, it wud be in 1,2 form. so we deduct 1 to make it 0,1 
y1 = y1-1

#Removing the response from train, else it will start it using as well for tree split
train$Delinquent = NULL
test$Delinquent = NULL

#XGB takes only matrices. All the categorical n
trainx = as.matrix(train)
testx = as.matrix(test)

library(Matrix)
library(data.table)
library(xgboost)




# Explaination of all the parameter: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md
# eta is [default=0.3, alias: learning_rate]step size shrinkage; used in update to prevents overfitting.
# max_dpeth is the maximum depth of a tree
# nround is the max no. of iterations or no. of trees to be added to the model.
# subsample is ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to
# grow trees and this will prevent overfitting. colsample_bytree [default=1] is subsample ratio of columns when constructing each tree.
# nthread [default to maximum number of threads available if not set] is the number of parallel threads used to run xgboost

# #Here The output that it'll give, is the classification error on the training data set.
xgb = xgboost(data = trainx, 
               label = y, 
               eta = 0.1,
               max_depth = 15, 
               nround=25, 
               subsample = 0.5,
               colsample_bytree = 0.4,
               seed = 1,
               eval_metric = "auc",
               objective = "binary:logistic",
               nthread = 3
)


# Gain is the improvement in accuracy brought by a feature to the branches it is on
# More details here:  https://github.com/dmlc/xgboost/blob/master/R-package/vignettes/discoverYourData.Rmd
importance = xgb.importance(feature_names = colnames(trainx), model = xgb)
head(importance)

xgb.plot.importance(importance_matrix = importance)

Pred = predict(xgb, testx)
summary(Pred)
table(y1, Pred>=0.5)

#ROC curve and AUC of the prediction
library(ROSE)
roc.curve(y1, Pred)


https://github.com/dmlc/xgboost/blob/master/demo/kaggle-otto/understandingXGBoostModel.Rmd

