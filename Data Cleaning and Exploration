
df11 = read.csv("C:\\Users\\sohail.ahmad\\Documents\\tata_old.csv")
df22 = read.csv("C:\\Users\\sohail.ahmad\\Documents\\tata_new.csv")

#Merging the two datasets using three common variables
data = merge(df11, df22, by =c("conno", "web_top_n", "ccid"))

#Removing the duplicate values
df = data[!duplicated(data),]


#Show only columns which have one or more missing values
M = sapply(df, function(x) sum(is.na(x)))
sort(M[M>0], decreasing=TRUE) #Sorting in descending order

#Remove all the columns which have over 4k missing values
df[colSums(is.na(df)) > 4000]  = NULL




###################    Imputation: Mean Value and MICE    ###########################

####  Imputation of max_od with the mean value
df1$max_od[is.na(df1$max_od)] = mean(df1$max_od, na.rm=TRUE)

MICE: Multivariate Imputation of Chained Equation. It works only when the data is MAR (missing at random). IN MAR, we assume that
the missing data depends upon the observed values and not on unobserved values. We can test the MAR using Little's Test of randomness
If the data doesnt follow the MAR, MICE will give out biased results. MICe automatically run Little's test and does 5 step imputation.

How does it work? Lets say there is a column AA which has missing values. Then MICE will run regression (logistic/linear) and use AA
as response and rest of the vars as features and then the missing values of AA will be replaced by predicted values. This is now done
for all the variables and we get imputed values.

Precisely, the methods used by this package are:
   --PMM (Predictive Mean Matching)  – For numeric variables
   --logreg(Logistic Regression) – For Binary Variables( with 2 levels)
   --polyreg(Bayesian polytomous regression) – For Factor Variables (>= 2 levels)
   --Proportional odds model (ordered, >= 2 levels)

In R:  library(mice);  imputed_df = complete(mice(df))



#######################       OUTLIER TREATMENT. Capping extreme values at 5% and 95%      ###############################
pcap <- function(x){
  for (i in which(sapply(x, is.numeric))) {
    quantiles <- quantile( x[,i], c(.05, .95 ), na.rm =TRUE)
    x[,i] = ifelse(x[,i] < quantiles[1] , quantiles[1], x[,i])
    x[,i] = ifelse(x[,i] > quantiles[2] , quantiles[2], x[,i])}
  x}

#Applying the percentile capping function on the dataset
df2 = pcap(df1)
summary(df2$Principal_Outstanding_Apr16)

#################   CROSS TABULATION    ###################

library(gmodels
CrossTable(df$Frequency, df$Chargeoff_Flag)



############################        MULTICOLLINEARITY CHECK     ####################################

We use VIF (variance inflation factors) and eigen values to check the multicollinearity in linear and logistic model
We can use correlation to check pairwise multicollineairty amongs variables


# Creating Correlation Matrix
df_num = df[,sapply(df, function(x) is.numeric(x))]
z = cor(df_num)
zdf = as.data.frame(as.table(z))  #COnverting matrix into table and then dataframe
zdf1 = subset(zdf, abs(Freq) > 0.5)  #Only taking values that are greater than absolute 0.5
zdf2 = subset(zdf1, abs(Freq) !=1)  #Removing correlation equal to since it is correlation of the same variable












