Both PCA & FA are used for dimensionality reduction and solve the problem of multi-collinearity.

PCA tries to find linear combination of the variables which contain much information by looking at the variance. It finds 
new combination of variables which form larger variances. These new set of variables are known as principle components. 
The direction of 1st principal component is such that it captures the maxim variance in the data. 2nd component is always 
orthogonal on the 1st principal component.

####################################             FACTOR ANALYSIS            ##############################################
Factor Analysis tries to find hidden variables which affect your observed variables by looking at the correlation. So letâ€™s 
 say there are three variables which are highly correlated. Factor analysis will club these variables together and create a 
 single factor (Variable).
We use Factor Analysis to remove the problem of multicollinearity in the data and convert high-dimensional data into low-dimension.
The factors in FA are the weights of diff variables together. Factors are calculated from correlation matrix.

Few Terms Explained:
FACTOR LOADINGS: Factor loading is basically the correlation coefficient for the variable and factor.  
EIGEN-VALUES: Eigenvalues is also called characteristic roots.  Eigenvalues shows variance explained by that particular factor out of 
             the total variance.
FACTOR SCORE: The factor score is also called the component score.  This score is of all row and columns, which can be used as an 
              index of all variables and can be used for further analysis.

ROTATION: In FA, a particular variable might heavily dominate a factor. We use rotation to smooth out the dominance.
          Suppose you've 10 variables. So, you can make 1st factor using 2 or variables which are highly correlated with each
          other. The factor analysis program then looks for the second set of correlations and calls it Factor 2, and so on. 
          Now these factors can be rotated along any axis to better fit the latent variables. While rotating, a particular 
          variables CAN OR CANT be correlated to other factors.
       OBLIQUE ROTATION: Rotations that allow factors to be correlated to each other is called oblique rotation.
       ORTHOGONAL ROTATION: When we rotate all factors by 90-degree so that these factors are uncorrelated to each other.
       Orthogonal rotation is of 3-types:
       
       1) QUARTIMAX: In each variable the large loadings are increased and the small ones are decreased so that each variable
                     will only load on a few factors.
       2) VARIMAX:   In each factor the large loadings are increased and the small ones are decreased so that each factor only
                     has a few variables with large loadings. Its basically variance maximization
       3) EQUIMAX:   To rotate the loadings so that a variable loads high on one factor but low on others
       4) Orthomax:  User determined, based on the specified value of gamma
                     


Three tests are conducted for the purpose of factor analysis:
1) Parallel Analysis: It uses eigen values and shows the np. of factors that has to be created.
2) Bartlett's test for Sphericity: Here we testif the correlation matrix is identity (diagnals is 1 and rest is zero). 
   Identity of correlation matrix means variables are indepedent. If p-value is zero, we'll reject null hypothesis means 
   there are some correlation between variables and we can go ahead with factor analysis.
3) Kaiser-Meyer-Olkin (KMO) Measure of Sample Adequacy (MSA): measure of how suited your data is for Factor Analysis. It 
   checks proportion of variance among variables that might be common variance (or correlations).
   KMO value b/w 0.8-1 means sampling is adequate. KMO less than 0.8 is not good and remedial action should be taken.


rm(list=ls())
df = read.csv("C:\\Users\\sohail.ahmad\\Documents\\tata_final.csv")

#Taking the subset of numerical values for analysis
df1 = df[,146:172]
summary(df1)


#Show only columns which have one or more missing values
M = sapply(df1, function(x) sum(is.na(x)))
sort(M[M>0], decreasing=TRUE) #Sorting in descending order



# PARALLEL ANALYSIS  using eigen values. We consider all the components who values is 1 or greater than 1. We'll take 5 
# factors in this case. eigen values explains the variance in the dataset
library(psych)
library(nFactors)
evs = eigen(cor(df1)) 
evs$values
ap = parallel(subject=nrow(df1),var=ncol(df1),rep=100,cent=.05)
nS = nScree(x=evs$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)      #Drawing a screeplot



# factanal fn performs maximum-likelihood factor analysis on a covariance matrix or data matrix. The rotation= options include 
# "varimax", "promax", and "none". Add the option scores="regression" or "Bartlett" to produce factor scores.
fit1 = factanal(df1, factors = 5, rotation = "varimax", scores="regression")
colnames(fit1$loadings) = c("Fac_1","Fac_2","Fac_3","Fac_4","Fac_5")
print(loadings(fit1), digits=2, cutoff=.2, sort=TRUE) 
names(fit1)
summary(fit1)

#We can add the scores in original set and use em (instead of originial variables) for modelling
df2 = cbind(df1, fit1$scores)
fix(df2)


# Getting correlation matrix
corMat = cor(df1)
print(corMat)

# Getting Rotated Matrix
library(GPArotation)
solution = fa(r = corMat, nfactors = 5, rotate = "oblimin", fm = "pa")
print(solution)


#Bartlett's test for sperificity. corMat is correlation matrix. p-value is of chi-sq. $df is degree of freedom. n sample size
to take
cortest.bartlett(corMat, n = 1000,diag=TRUE)


#KMO measurement of sample of Adequacy. Here MSA means measurement of sample adequacy. corMat is correlation matrix
KMO(corMat)

## More here: http://www.theanalysisfactor.com/rotations-factor-analysis/
              http://www.statisticssolutions.com/factor-analysis-sem-exploratory-factor-analysis/
